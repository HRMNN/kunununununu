{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Untitled Kununu Project\n",
    "<b>Project Goal:</b> The automatic analysis of employer evaluations from the kununu portal. Although the code shall be scalable and flexible, it shall at this moment focuses on:<br>\n",
    "- One Company, and<br>\n",
    "- German Language Reviews<br>\n",
    "- The Feature \"Recomandation\".<br><br>\n",
    "\n",
    "<b>Strategic Goal:</b> The strategic goal pursued by this Project is to:\n",
    "- excercise, and<br>\n",
    "- flex<br>\n",
    "\n",
    "my Data Science muscles.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">\n",
    "<b>Next Steps</b><br>\n",
    "- Find Pun for Project-Title<br>\n",
    "- Write a Readme<br>\n",
    "- Add Pictures/ Examples/ Comments to guide Reader through Notebook<br>\n",
    "- Get Peer Feedback<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00 : Set Up Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00-01 : Import Libraries\n",
    "<span style=\"color: red;\">\n",
    "<b>Next Steps</b><br>\n",
    "- Consider: https://stackoverflow.com/questions/43377265/determine-if-text-is-in-english/48436520#48436520<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vectors, Matrices and thier Math\n",
    "import numpy as np\n",
    "# DataFrames\n",
    "import pandas as pd\n",
    "# Working with HTML Files\n",
    "from lxml import html\n",
    "# Turn HTML Element into string\n",
    "from lxml.etree import tostring\n",
    "# Basic Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "# Fancy Visualizations\n",
    "import seaborn as sns\n",
    "# Identify Language\n",
    "from langdetect import detect\n",
    "# Natural Language Handling\n",
    "import nltk\n",
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "# Tokenize Words\n",
    "from nltk import word_tokenize\n",
    "# How often does which token appear in a list\n",
    "from collections import Counter\n",
    "# Regular Expressions\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00-02 : Get additional Data for Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00-03 : Homemade Functions\n",
    "<br>\n",
    "<span style=\"color: red;\">\n",
    "<b>Next Steps</b><br>\n",
    "- Polish Code transform function<br>\n",
    "- Functions that are only used in one Chapter shall be defined there<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes evaluation-string and turns it into a Dataframe with the right format\n",
    "def transform(review):\n",
    "    # Set up Collector for Results\n",
    "    collector_grades = pd.DataFrame([])\n",
    "    # Loop through Categories within review\n",
    "    for category in review:\n",
    "        # Extract Title of Category\n",
    "        title = category[0]\n",
    "        # Extract written Text; if empty, turn to missing value\n",
    "        text  = category[1]\n",
    "        if text == []:\n",
    "            #text = np.nan    \n",
    "            text = None\n",
    "        # Extract Number of Stars\n",
    "        grade = category[2]\n",
    "        # Shape information into DataFrame        \n",
    "        result = pd.DataFrame([text,grade]).T\n",
    "        # Name the Columns (\"_T\" short for text, \"_N\" for number)\n",
    "        result.columns = [str(title+\"_N\"),str(title+\"_T\")]\n",
    "        # Drop empty columns\n",
    "        result.dropna(axis = 1, how = 'any', thresh = None, subset = None, inplace = True)        \n",
    "        # Join with allready collected evaluation from this review\n",
    "        collector_grades = pd.concat([collector_grades, result], axis = 1)\n",
    "    # Return results\n",
    "    return(collector_grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes requested result out of the table with the data-reconaissance results and turns it into a HTML expression\n",
    "#(IOT save time by not typing the whole expression)\n",
    "def xpr(tag):\n",
    "    cat  = recon.loc[tag].type\n",
    "    code = recon.loc[tag].code\n",
    "    text = recon.loc[tag].text\n",
    "    # Takes Category and code and turns it into a string with the right format\n",
    "    result = str('//'+cat+'[@class=\\\"'+code+'\\\"]')\n",
    "    # If requested (text == 1), the code to extract the text will be added\n",
    "    if text == 1:\n",
    "        result += \"/text()\"\n",
    "    # Return the Results\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00-04 : Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Color Scheme\n",
    "flatui = [\"lightgreen\", 'tomato']\n",
    "knncol = sns.color_palette(flatui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 : Data Mining\n",
    "<br>\n",
    "<span style=\"color: red;\">\n",
    "<b>Next Steps</b><br>\n",
    "- Scrape directly from WebPage<br>    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'kununu3.html', \"r\", encoding='utf-8') as f:\n",
    "    page = f.read()\n",
    "raw_html = html.fromstring(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 : Data Cleaning\n",
    "<br>\n",
    "<span style=\"color: red;\">\n",
    "<b>Next Steps</b><br>\n",
    "- What does \"grades.values.tolist()\" do again?!<br>\n",
    "- inputformat Text (unicode)<br>\n",
    "- Feature Formats<br>\n",
    "- Ferature Name/ Dictionary<br>\n",
    "- Comment Code<br>\n",
    "- Polish Code<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02-01 : Data Reconnaissance\n",
    "<br>\n",
    "<span style=\"color: red;\">\n",
    "<b>Next Steps</b><br>\n",
    "- Write Prose about the findings, leading to the Results in the DataFrame<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the Data Source - which is in HTML - I found the necessary tags to extract the Data. I turn these into a DataFrame, so I can use the tags (utilizing the xpr-function from 00-03) to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Results from Data Reconnaissance into a DataFrame\n",
    "recon = pd.DataFrame([['article','index__contentBlock__7vKo-',0],\n",
    "                      ['h3','index__title__2uQec',1],\n",
    "                      ['span','index__score__16yy9',1],\n",
    "                      ['span','index__recommendation__jftd3',1],\n",
    "                      ['span','index__position__mCyeO',1],\n",
    "                      ['span','index__sentence__3PKUg index__middot__3vlu3',1],\n",
    "                      ['div','index__factor__3Z15R index__factor__3YJYJ index__expanded__349V7',0],\n",
    "                      ['h4','index__title__W4hOp',1],\n",
    "                      ['p','index__plainText__lgNCM',1],\n",
    "                      ['span','index__stars__2ads4 index__medium__1wpWb index__stars__3lgvx',0],\n",
    "                      ['div','index__factor__3Z15R',0]])\n",
    "# Name Features\n",
    "recon.columns =[\"type\",\"code\",\"text\"]\n",
    "# Name the Tags\n",
    "recon.index = ['review','headline','score','recom','status','department',\n",
    "               'cat_one','cat_one_title','cat_one_text','cat_one_grade',\n",
    "               'cat_two']\n",
    "# Show Result\n",
    "recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpr('cat_one_grade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02-02 : Shape the Data into DataFrame\n",
    "<br>\n",
    "<span style=\"color: red;\">\n",
    "<b>Next Steps</b><br>\n",
    "- Combine Tag One and Tag Two Loop<br>\n",
    "    \n",
    "- Extract Time (    # Extract the Time the Review was given\n",
    "   \n",
    "    \n",
    "    try:\n",
    "        # If time is given\n",
    "        time = [''.join(review_html.xpath(xpr('time','index__date__37sgB',0))[0].attrib.get('datetime'))]\n",
    "       except:\n",
    "        # If not, save 0\n",
    "        time = '0'    )\n",
    "    \n",
    "- utilize list expressions<br>\n",
    "- improve performance (!!!)<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set up Collector for overall result\n",
    "overview = pd.DataFrame([])\n",
    "\n",
    "# Loop Through Reviews in the HTML code\n",
    "for idx in raw_html.xpath(xpr('review')):\n",
    "    # Select individual review \n",
    "    review_html = html.fromstring(tostring(idx))\n",
    "    \n",
    "    # Extract the Headline from the Review\n",
    "    headline   = review_html.xpath(xpr('headline'))\n",
    "    # Extract the Average Score\n",
    "    score      = review_html.xpath(xpr('score'))\n",
    "        \n",
    "    # Extract the Recommendation\n",
    "    recom      = review_html.xpath(xpr('recom'))    \n",
    "    # Extract the Status of the reviewer   \n",
    "    status     = [''.join(review_html.xpath(xpr('status')))]\n",
    "    # Extract the department of the reviewer      \n",
    "    department = review_html.xpath(xpr('department'))    \n",
    "\n",
    "    # Set up Collector for Categories within the review   \n",
    "    evaluation = pd.DataFrame([])\n",
    "    \n",
    "    # Review the categories Tag One\n",
    "    categories_one = review_html.xpath(xpr('cat_one'))    \n",
    "    # Loop through these categories\n",
    "    for idx in categories_one:\n",
    "        # Select category\n",
    "        category      = html.fromstring(tostring(idx))\n",
    "        # Extract Title of Category\n",
    "        cat_one_title = str(category.xpath(xpr('cat_one_title'))[0])\n",
    "        # Extract Evaluation Text of Category        \n",
    "        cat_one_text  = str(category.xpath(xpr('cat_one_text')))             \n",
    "        # Change Missing Value Format\n",
    "        if cat_one_text == '[]':\n",
    "            cat_one_text = np.nan   \n",
    "        \n",
    "        # Try to extract grade, if it extists           \n",
    "        try:\n",
    "            cat_one_grade = int(category.xpath(xpr('cat_one_grade'))[0].attrib.get('data-score'))\n",
    "        except:\n",
    "            cat_one_grade = np.nan\n",
    "        # Turn Results into DataFrame\n",
    "        summary = pd.DataFrame([cat_one_title, cat_one_grade, cat_one_text]).T    \n",
    "        # Add to the List\n",
    "        evaluation = evaluation.append(summary) \n",
    "        \n",
    "    # Review the categories Tag Two\n",
    "    categories_two = review_html.xpath(xpr('cat_two'))          \n",
    "    for idx in categories_two:\n",
    "        category     = html.fromstring(tostring(idx))\n",
    "        # Extract Title of Category\n",
    "        cat_two_title = str(category.xpath(xpr('cat_one_title'))[0])\n",
    "        # Extract Evaluation Text of Category        \n",
    "        cat_two_text  = str(category.xpath(xpr('cat_one_text'))[0])               \n",
    "        # Change Missing Value Format\n",
    "        if cat_two_text == '[]':\n",
    "            cat_two_text = np.nan           \n",
    "       \n",
    "        # Try to extract grade, if it extists           \n",
    "        try:\n",
    "            cat_two_grade = int(category.xpath(xpr('cat_one_grade'))[0].attrib.get('data-score'))\n",
    "        except:\n",
    "            cat_two_grade = np.nan\n",
    "        # Turn Results into DataFrame\n",
    "        summary = pd.DataFrame([cat_two_title, cat_two_grade, cat_two_text]).T    \n",
    "        # Add to the List\n",
    "        evaluation = evaluation.append(summary)        \n",
    "        \n",
    "    # Reset Index of collected evaluations\n",
    "    evaluation.reset_index(drop = True, inplace = True)\n",
    "    # and turn into list for easy handling\n",
    "    evaluation = evaluation.values.tolist()\n",
    "    \n",
    "    # Put extracted Information from review into DataFrame format; score turned to list\n",
    "    summary = pd.DataFrame([headline, \n",
    "                             [score], recom, status, department, \n",
    "                             pd.Series([evaluation])]).T\n",
    "    # and add it to the others\n",
    "    overview = overview.append(summary)\n",
    "\n",
    "# Reset Index\n",
    "overview.reset_index(drop = True, inplace = True)\n",
    "# IOT drop first review (a dummy)\n",
    "overview.drop([0], inplace = True)\n",
    "# and finalize the index\n",
    "overview.reset_index(drop = True, inplace = True)\n",
    "# name the features\n",
    "overview.columns = [\"headline\",\"avg_score\",\"recom\",\"status\",\"depart\",\"evaluations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Collector for results\n",
    "collector_reviews = pd.DataFrame([])\n",
    "# loop through reviews\n",
    "for review in overview.evaluations:\n",
    "    # If existing ...  \n",
    "    if(review != None):\n",
    "        # change into DataFrame using 00-03's transform function and add to other results\n",
    "        collector_reviews = pd.concat([collector_reviews, \n",
    "                                       transform(review)],\n",
    "                                      axis = 0, \n",
    "                                      sort = False,\n",
    "                                      join = 'outer')\n",
    "# reset index\n",
    "collector_reviews.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Attach to newly formatted to core DataFrame, and\n",
    "overview = pd.concat([overview, collector_reviews], axis=1)\n",
    "# drop original feature\n",
    "overview.drop('evaluations', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02-03 : Change Feature Formats\n",
    "<br>\n",
    "<span style=\"color: red;\">\n",
    "<b>Next Steps</b><br>\n",
    "- Integrate this into 02-02<br>\n",
    "- time pd.to_datetime(overview.date)<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview.headline  = overview.headline.astype(str)\n",
    "overview.recom     = overview.recom.astype(str)\n",
    "overview.status    = overview.status.astype(str)\n",
    "overview.depart    = overview.depart.astype(str)\n",
    "for idx in [s for s in overview.columns[6:] if \"_T\" in s]:\n",
    "    overview[idx] = overview[idx].astype(str)\n",
    "for idx in [s for s in overview.columns[6:] if \"_N\" in s]:\n",
    "    overview[idx] = overview[idx].replace(',','.').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview.avg_score = [float(idx[0].replace(',','.')) for idx in overview.avg_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02-04 : Extract additional Information from Features\n",
    "<br>\n",
    "<span style=\"color: red;\">\n",
    "<b>Next Steps</b><br>\n",
    "- Change into categorical feature if applicable<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02-04-01 : Status -> Current or Former Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview[\"ex\"] = overview.status.str.contains(\"Ex-\", regex = False)\n",
    "overview[\"status\"] = overview.status.str.replace(\"Ex-\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02-04-02 : Department -> Location, Department, Division, Time, Vocational Training\n",
    "<br>\n",
    "<span style=\"color: red;\">\n",
    "<b>Next Steps</b><br>\n",
    "- Add to Data Reconaissance<br>\n",
    "- Remove the \\n earlier<br>\n",
    "- Polish and Comment Code<br>\n",
    "- Work with Regex/ str.split to make this safe<br>\n",
    "- Extract Time of Review in 02-02, use here to get time<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_check(sublist,stringz):\n",
    "    try:\n",
    "        return(sublist.index(stringz))+1\n",
    "    except:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_take(sublist,number):\n",
    "    if number != None:\n",
    "        return(sublist[number])\n",
    "    else:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \\n\n",
    "overview.depart = overview.depart.str.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize IAW results from Data Reconaissance\n",
    "deptoken = [re.split(r'( zum/zur )|( bis )|( bei )|( in )|( im Bereich )',idx) for idx in overview.depart]\n",
    "# Remove Nones\n",
    "deptoken = [[i for i in sublist if i]\n",
    "         for sublist in deptoken]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Location (after the ' in ' bracket)\n",
    "location = [sublist[sublist.index(' in ')+1] for sublist in deptoken]\n",
    "# Remove everything that is not the location\n",
    "location = [idx.replace(\" gearbeitet.\",'') for idx in location]\n",
    "location = [idx.replace(\" abgeschlossen.\",'') for idx in location]\n",
    "location = [idx.replace(\" absolviert.\",'') for idx in location]\n",
    "# Add to Main DataFrame\n",
    "overview['location'] = location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Division (after the ' bei ' bracket)\n",
    "division = [sublist[sublist.index(' bei ')+1] for sublist in deptoken]\n",
    "overview['division'] = division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Department\n",
    "delta = [save_check(sublist,' im Bereich ') for sublist in deptoken]\n",
    "department = [save_take(deptoken[idx], delta[idx]) for idx in range(0, len(overview.depart))]\n",
    "overview[\"department\"] = department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02-04-03 : Review-Texts -> Language\n",
    "<br>\n",
    "<span style=\"color: red;\">\n",
    "<b>Next Steps</b><br>\n",
    "- Polish this<br>\n",
    "- What happens if all values are missing?<br>\n",
    "- text_feats earlier<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_detect(string):\n",
    "    try:\n",
    "        return(detect(string))\n",
    "    except:\n",
    "        return('UKN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All text Features\n",
    "text_feats = ['headline']\n",
    "text_feats += [idx for idx in overview.columns\n",
    "if '_T' in idx]\n",
    "# Add all written text\n",
    "text = [[overview[feat][review] for feat in text_feats\n",
    "if overview[feat][review] != 'nan'] for review in range(0,overview.shape[0])] \n",
    "# Join all text\n",
    "text = [\" \".join(idx) for idx in text]\n",
    "# Add to DataFrame\n",
    "overview['text'] = text\n",
    "# Delete '\\n'\n",
    "overview['text'] = overview['text'].str.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview['language'] = [safe_detect(idx) for idx in overview.text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02-04 : Drop irrelevant Data\n",
    "<i> Since this project focuses on the recommendation, all observations without one will be dropped.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview.drop(overview[overview.recom == 'None'].index, inplace = True)\n",
    "overview = overview.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Warning!</b> Once the the Scope of this Project is extended and the observations are not dropped anymore - the code under 02-02 needs to be altered. At earlier times, when kununu did not have the recommendation feature, the ratings where tagged differently. The current choice of scope convinently circumvents this issue.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 : Analysis\n",
    "<br>\n",
    "<span style=\"color: red;\">\n",
    "<b>Next Steps</b><br>\n",
    "- Automatic Identification of \"Grading-Curve\"'s (Unsupervised Learning: DBSCAN?)<br>\n",
    "- Extract Time Feature<br>\n",
    "- Make, and write down, conclusions<br>\n",
    "- unify design/ colorscheme<br>\n",
    "</span>\n",
    "<br>\n",
    "<i> Focus of this Project is the reviewer's recommendation. Every piece of Data is seen through this lense therefore. The analysis of each single features by itself is therfore skipped. Exception is the Guest of Honor itself: The Recommendation:</i> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(overview.recom.value_counts(),labels = overview.recom.value_counts().index, autopct='%1.1f%%', colors = flatui)\n",
    "plt.title(\"Recommendation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03-01 : Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03-01-01 : Average Review Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(1,5,10)\n",
    "sns.distplot(overview.avg_score[overview.recom==\"Empfohlen\"] , color=\"lightgreen\", label=\"Recommended\", kde = False, bins = bins)\n",
    "sns.distplot(overview.avg_score[overview.recom==\"Nicht empfohlen\"] , color=\"tomato\", label=\"Not Recommended\", kde = False, bins = bins)\n",
    "plt.xlabel(\"Average Score\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.title(\"Kununu Reviews\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03-01-02 : Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0.5,5.5,6)\n",
    "for idx in (overview.columns[overview.dtypes == float].drop(\"avg_score\")):\n",
    "    plt.title(str(idx))\n",
    "    data = overview[idx]\n",
    "    sns.distplot(data[overview.recom==\"Empfohlen\"] , color=\"lightgreen\", label=\"Recommended\", kde = False, bins = bins)\n",
    "    sns.distplot(data[overview.recom==\"Nicht empfohlen\"] , color=\"tomato\", label=\"Not Recommended\", kde = False, bins = bins)\n",
    "    plt.xlabel(\"Average Score\")\n",
    "    plt.ylabel(\"Number of Reviews\")\n",
    "    plt.title(idx.replace(\"_N\",\"\"))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03-02 : Categorical Features\n",
    "<br>\n",
    "<span style=\"color: red;\">\n",
    "<b>Next Steps</b><br>\n",
    "- Auto Combine Small/ Irrelevant<br>\n",
    "- Colorscheme for Piecharts<br>\n",
    "- Argh. Red/ Green switches!!<br>\n",
    "- Make Functions out of this<br>\n",
    "</span>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03-02-01 : Former Worker / Current Worker\n",
    "<br>\n",
    "<span style=\"color: red;\">\n",
    "<b>Next Steps</b><br>\n",
    "- Phrase Better<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(overview.ex.value_counts(),labels = overview.ex.value_counts().index, autopct='%1.1f%%')\n",
    "plt.title(\"Reviews by Former Employees vs Current Employes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\"recom\", data = overview, kind = \"count\", col=\"ex\", palette = knncol, alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03-02-02 : Type of Contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(overview.status.value_counts(),labels = overview.status.value_counts().index, autopct='%1.1f%%')\n",
    "plt.title(\"Reviews by Type of Contract\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\"recom\", data = overview, kind = \"count\", col=\"status\", palette = knncol, col_wrap=2, alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03-02-03 : Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(overview.location.value_counts(),labels = overview.location.value_counts().index, autopct='%1.1f%%')\n",
    "plt.title(\"Reviews by Location\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.catplot(\"recom\", data = overview, kind = \"count\", col=\"location\", palette = knncol, col_wrap=2, alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03-02-04 : Division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(overview.division.value_counts(),labels = overview.division.value_counts().index, autopct='%1.1f%%')\n",
    "plt.title(\"Reviews by Division\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.catplot(\"recom\", data = overview, kind = \"count\", col=\"division\", palette = knncol, col_wrap=2, alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03-02-05 : Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(overview.department.value_counts(),labels = overview.department.value_counts().index, autopct='%1.1f%%')\n",
    "plt.title(\"Reviews by Department\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.catplot(\"recom\", data = overview, kind = \"count\", col=\"department\", palette = knncol, col_wrap=2, alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03-02-06 : Language\n",
    "Not part of Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(overview.language.value_counts(),labels = overview.language.value_counts().index, autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> langdetect is not perfect - but it can only work with what it is fed. I am highly certain that the 'de' and 'en' are correct - and only those</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.catplot(\"recom\", data = overview, kind = \"count\", col=\"language\", palette = knncol, col_wrap=2, alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03-02 : Textual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03-02-01 : Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Delete non German observation\n",
    "overview = overview[overview.language == 'de']\n",
    "overview.reset_index(drop = True, inplace = True)\n",
    "overview.text[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Words\n",
    "tokenized = [word_tokenize(idx) for idx in overview.text]\n",
    "tokenized[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave numbers and punctuation behind\n",
    "tokenized = [[idx_2 for idx_2 in idx\n",
    "              if idx_2.isalpha()]\n",
    "             for idx in tokenized]\n",
    "tokenized[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# All lowercase\n",
    "tokenized = [[idx_2.lower() for idx_2 in idx]\n",
    "             for idx in tokenized]\n",
    "tokenized[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out Stopwords\n",
    "tokenized = [[idx_2 for idx_2 in idx\n",
    "              if idx_2 not in stopwords.words('german')]\n",
    "             for idx in tokenized]\n",
    "tokenized[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03-02-02 : Bag of Words\n",
    "<br>\n",
    "<span style=\"color: red;\">\n",
    "<b>Next Steps</b><br>\n",
    "- Rethink how to handle categories like \"Gut am Arbeitgeber finde ich\" / \"Verbesserungsvorschläge\" -> Employee Sentiment is in the open here<br>\n",
    "- Apply Seaborn<br>\n",
    "- COMMENT!<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 03-02-02-01 : All Text\n",
    "Look at all the company's reviews as one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all Tokens\n",
    "allwords = [val for sublist in tokenized for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords_Counter = Counter(allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(y = [idx[0] for idx in allwords_Counter.most_common(20)],\n",
    "         width = np.array([idx[1] for idx in allwords_Counter.most_common(20)])/len(allwords))\n",
    "plt.xticks(rotation = 90)\n",
    "plt.title(\"Token Frequency, all Text\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 03-02-02-02 : By Recommandation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_texts = [tokenized[idx] for idx in range(0,len(tokenized))\n",
    " if overview.recom[idx] == \"Nicht empfohlen\"]\n",
    "minus_words = [val for sublist in minus_texts for val in sublist]\n",
    "minus_count = Counter(minus_words).most_common(20)\n",
    "plt.barh(y = [idx[0] for idx in minus_count], width = np.array([idx[1] for idx in minus_count])/len(minus_words), color = \"tomato\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.title(\"Word Frequency, Not Recommended\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plus_texts = [tokenized[idx] for idx in range(0,len(tokenized))\n",
    " if overview.recom[idx] == \"Empfohlen\"]\n",
    "plus_words = [val for sublist in plus_texts for val in sublist]\n",
    "plus_count = Counter(plus_words).most_common(20)\n",
    "plt.barh(y = [idx[0] for idx in plus_count], width = np.array([idx[1] for idx in plus_count])/len(plus_words), color = \"lightgreen\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.title(\"Word Frequency, Recommended\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allall = pd.DataFrame.from_dict(Counter(allwords), orient='index')/len(allwords)\n",
    "plus = pd.DataFrame.from_dict(Counter(plus_words), orient='index')/len(plus_words)\n",
    "minus = pd.DataFrame.from_dict(Counter(minus_words), orient='index')/len(minus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector_reviews = pd.concat([allall.T, \n",
    "                               minus.T],\n",
    "                              axis = 0, \n",
    "                              sort = False,\n",
    "                              join = 'outer')\n",
    "collector_reviews = pd.concat([collector_reviews, \n",
    "                               plus.T],\n",
    "                              axis = 0, \n",
    "                              sort = False,\n",
    "                              join = 'outer')\n",
    "\n",
    "collector_reviews = collector_reviews.fillna(0)\n",
    "collector_reviews = collector_reviews.astype(float)\n",
    "collector_reviews.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plus_topflop = pd.DataFrame(collector_reviews.loc[2] - collector_reviews.loc[0]).sort_values(by = 0).head(10)\n",
    "plus_topflop = plus_topflop.append(pd.DataFrame(collector_reviews.loc[2] -collector_reviews.loc[0]).sort_values(by = 0).tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(y = plus_topflop.index, width = plus_topflop[0], color = \"lightgreen\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.title(\"Top/ Flop 10: relative Word Frequency, Recommended\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_topflop = pd.DataFrame(collector_reviews.loc[1] - collector_reviews.loc[0]).sort_values(by = 0).head(10)\n",
    "minus_topflop = minus_topflop.append(pd.DataFrame(collector_reviews.loc[1] -collector_reviews.loc[0]).sort_values(by = 0).tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(y = minus_topflop.index, width = minus_topflop[0], color = 'tomato')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.title(\"Top/ Flop 10: relative Word Frequency, Not Recommended\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YY - Construction Site starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03-02-05 : TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = Dictionary(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dct.doc2bow(line) for line in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [model[corpus[idx]] for idx in range(0,len(corpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in weights for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [len(idx) for idx in weights]\n",
    "b = [idx for idx in range(0,len(weights))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taser = pd.DataFrame(np.repeat(b, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taser.columns = [\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taser[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taser = pd.DataFrame(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taser[\"text\"] = pd.DataFrame(np.repeat(b, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taser.columns = [\"id\",\"weight\",\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest = min(taser.groupby(\"text\").max().weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(taser.weight >= lowest) / len(taser.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(taser.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Counter([dct.get(idx) for idx in taser[taser.weight>lowest].reset_index(drop = True).id]).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(y = [idx[0] for idx in test], width = np.array([idx[1] for idx in test]))\n",
    "plt.xticks(rotation = 90)\n",
    "plt.title(\"Word Frequency, all Text\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZZ - Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "page = requests.get('http://econpy.pythonanywhere.com/ex/001.html')\n",
    "tree = html.fromstring(page.content)\n",
    "#This will create a list of buyers:\n",
    "buyers = tree.xpath('//div[@title=\"buyer-name\"]/text()')\n",
    "#This will create a list of prices\n",
    "prices = tree.xpath('//span[@class=\"item-price\"]/text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = game.xpath('.//span[contains(@class, \"platform_img\")]')\n",
    "# !! CONTAINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unternehmenslink anwählen\n",
    "- Anzahl der Bewertungen einlesen\n",
    "- Durch 10 Teilen = XYRPP\n",
    "-- einlesen der daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://python-graph-gallery.com/100-calling-a-color-with-seaborn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrlist(data):\n",
    "    c = data.corr()\n",
    "    s = c.unstack()\n",
    "    t = s[s != 1]\n",
    "    return(t.sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment on time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depar ?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.youtube.com/results?search_query=python+text+analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/jbesomi/texthero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://seaborn.pydata.org/tutorial/color_palettes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "https://medium.com/@ishan16.d/text-classification-in-python-with-scikit-learn-and-nltk-891aa2d0ac4b\n",
    "https://nlpforhackers.io/text-classification/\n",
    "https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 : Predictive Modeling\n",
    "<br>\n",
    "<span style=\"color: red;\">\n",
    "<b>Next Steps</b><br>\n",
    "- Write Demonstrator: Enter a Text, Get a Guess if Recom/Not Recom<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 : Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05-01 : TITLE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05-02 : Future Works\n",
    "#### 05-02-00 : General Matters\n",
    "- Contact XING SE and check legal, before publishing<br>\n",
    "\n",
    "#### 05-02-01 : Data Mining<br>\n",
    "\n",
    "#### 05-02-02 : Data Cleaning<br>\n",
    "\n",
    "#### 05-02-03 : Data Analysis<br>\n",
    "- In Depth Analysis beyond the Recommendation<br>\n",
    "- Seperate Notebook for Analysis<br>\n",
    "- Comparing multiple companies<br>\n",
    "- Text vs No-Text<br>\n",
    "- Text Lenght<br>\n",
    "\n",
    "#### 05-02-04 : Predictive Modeling<br>\n",
    "\n",
    "#### 05-02-05 : Data Visualization<br>\n",
    "- Automatic generated DINA4 pdf with a Summary Report<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
